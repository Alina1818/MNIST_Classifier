# -*- coding: utf-8 -*-
"""MnistClassifierInterface.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hOLgaWciVSmoBtgbm3ti-N-r-3V29X3S

# MNIST Classifier Demo
This notebook demonstrates the implementation of three MNIST classifiers:
1. Random Forest
2. Feed-Forward Neural Network
3. Convolutional Neural Network (CNN)

Each model implements the `MnistClassifierInterface` with methods:
- `train`
- `predict`
"""

# ================================
# 1. Import libraries
# ================================

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import copy
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report
from abc import ABC, abstractmethod

# ================================
# 2. Data preparation
# ================================

# Transform images to tensors for PyTorch models
transform = transforms.ToTensor()

# Download MNIST dataset
train_dataset = datasets.MNIST(root="data", train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root="data", train=False, download=True, transform=transform)

# DataLoader for NN and CNN
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# For RandomForest — convert to numpy arrays
X_train_rf = train_dataset.data.view(-1, 28*28).numpy().astype(np.float32) / 255.0
y_train_rf = train_dataset.targets.numpy().astype(np.int64)
X_test_rf  = test_dataset.data.view(-1, 28*28).numpy().astype(np.float32) / 255.0
y_test_rf  = test_dataset.targets.numpy().astype(np.int64)

# ================================
# 3. MnistClassifierInterface
# ================================
class MnistClassifierInterface(ABC):
    @abstractmethod
    def fit(self, train_data, train_labels=None):
        pass

    @abstractmethod
    def predict(self, x):
        pass

# ================================
# 4. Random Forest Model with Randomized Search
# ================================

# Random search parameters
param_dist = {
    'n_estimators': [150, 200],
    'max_depth': [None, 30],
    'max_features': ['sqrt'],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True],
    'criterion': ['gini', 'entropy']
}

# Model
rf = RandomForestClassifier(random_state=42)

# Using RandomizedSearchCV is faster because it only tests n_iter combinations
random_search = RandomizedSearchCV(estimator=rf,
                                   param_distributions=param_dist,
                                   n_iter=20,  # 20 random combinations
                                   cv=3,
                                   scoring='accuracy',
                                   verbose=1,
                                   n_jobs=-1,
                                   random_state=42)

# Model training
random_search.fit(X_train_rf, y_train_rf)

# Result
print("The best parameters:", random_search.best_params_)
print("Best accuracy (CV):", random_search.best_score_)

# Test set score
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test_rf)
test_acc = accuracy_score(y_test_rf, y_pred)
print(f"Accuracy on the test: {test_acc:.4f}")

# ================================
# Random Forest Implementation
# ================================

class RandomForestMnist(MnistClassifierInterface):
    def __init__(self):
        self.model = RandomForestClassifier(
            n_estimators=150, min_samples_split=2, min_samples_leaf=1,
            max_features='sqrt', max_depth= None,
            criterion='gini', bootstrap=True,
            random_state=42
        )

    def fit(self, train_data, train_labels=None, **kwargs):
        if train_labels is None:
            raise ValueError("train_labels required for RandomForestMnist.fit")
        self.model.fit(train_data, train_labels)

    def predict(self, x, **kwargs):
        return self.model.predict(x)

# ================================
# 5. Base class for trainable PyTorch models with training, validation, and early stopping
# ================================

class TrainableNN(nn.Module):
    def __init__(self):
        super().__init__()

    def fit(self, train_data, val_data=None, epochs=50, lr=0.001, device="cpu", early_stopping_patience=5):
        self.to(device)
        optimizer = torch.optim.Adam(self.parameters(), lr=lr)
        loss_fn = nn.CrossEntropyLoss()

        best_loss = float('inf')
        best_model_wts = copy.deepcopy(self.state_dict())
        patience_counter = 0

        for epoch in range(epochs):
            # Train
            self.train()
            running_loss = 0.0
            num_batches = 0
            for images, labels in train_data:
                images, labels = images.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = self(images)
                loss = loss_fn(outputs, labels)
                loss.backward()
                optimizer.step()
                running_loss += loss.item()
                num_batches += 1
            train_loss = running_loss / max(1, num_batches)

            # Validation
            if val_data is not None:
                self.eval()
                val_loss = 0.0
                val_batches = 0
                with torch.no_grad():
                    for images, labels in val_data:
                        images, labels = images.to(device), labels.to(device)
                        outputs = self(images)
                        val_loss += loss_fn(outputs, labels).item()
                        val_batches += 1
                val_loss /= max(1, val_batches)

                print(f"Epoch [{epoch+1}/{epochs}] — Train Loss: {train_loss:.4f} — Val Loss: {val_loss:.4f}")

                # Save best
                if val_loss < best_loss:
                    best_loss = val_loss
                    best_model_wts = copy.deepcopy(self.state_dict())
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= early_stopping_patience:
                        print(f"Early stopping at epoch {epoch+1}")
                        break
            else:
                print(f"Epoch [{epoch+1}/{epochs}] — Train Loss: {train_loss:.4f}")

        # Restore best
        self.load_state_dict(best_model_wts)
        print(f"\nRestored best model (Val Loss: {best_loss:.4f})")

    def predict(self, x, device="cpu"):
        # Make predictions on a batch or full dataset
        self.eval()
        self.to(device)
        x = x.to(device)
        with torch.no_grad():
            outputs = self(x)
            return torch.argmax(outputs, dim=1)

# ================================
# 6. Feed-Forward Neural Network
# ================================

class FeedForwardNN(TrainableNN):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 512)
        self.bn1 = nn.BatchNorm1d(512)
        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)
        self.fc3 = nn.Linear(256, 10)
        self.dropout = nn.Dropout(0.3)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = F.relu(self.bn1(self.fc1(x)))
        x = self.dropout(x)
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout(x)
        return self.fc3(x)

# 3тя спроба покращити результат
class CNNClassifierKerasStyle(TrainableNN):
    def __init__(self):
        super().__init__()
        # --- Convolutional layers ---
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)

        # --- Pooling ---
        self.pool = nn.MaxPool2d(2, 2)

        # --- Dropout ---
        self.dropout_conv = nn.Dropout(0.25)
        self.dropout_fc = nn.Dropout(0.3)

        # --- Fully connected ---
        self.fc1 = nn.Linear(128 * 7 * 7, 256)
        self.fc2 = nn.Linear(256, 10)
        self.dropout_fc2 = nn.Dropout(0.2)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = self.dropout_conv(x)
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        x = self.dropout_conv(x)

        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = self.dropout_fc(x)
        x = self.dropout_fc2(x)
        x = self.fc2(x)
        return x

# ================================
# 7. Convolutional Neural Network
# ================================

class CNNClassifierKerasStyle(TrainableNN):
    def __init__(self):
        super().__init__()
        # Conv layers
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)

        # Pooling
        self.pool = nn.MaxPool2d(2, 2)

        # Dropout
        self.dropout_conv = nn.Dropout(0.25)
        self.dropout_fc = nn.Dropout(0.5)

        # Fully connected
        self.fc1 = nn.Linear(128 * 7 * 7, 256)  # 128 канали × 7 × 7
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 28 → 14
        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 14 → 7
        x = F.relu(self.bn3(self.conv3(x)))             # 7 → 7
        x = self.dropout_conv(x)
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = self.dropout_fc(x)
        return self.fc2(x)

# ==================
# 8. MnistClassifier
# ==================

class MnistClassifier:
    def __init__(self, algorithm):
        if algorithm == "rf":
            self.model = RandomForestMnist()
        elif algorithm == "nn":
            self.model = FeedForwardNN()
        elif algorithm == "cnn":
            self.model = CNNClassifierKerasStyle()
        else:
            raise ValueError("Unknown algorithm")

    def fit(self, train_data, train_labels=None, **kwargs):
        device = kwargs.get("device", "cpu")
        if isinstance(self.model, nn.Module):
            self.model.to(device)
            self.model.fit(
                train_data,
                val_data=kwargs.get("val_data"),
                epochs=kwargs.get("epochs", 50),
                lr=kwargs.get("lr", 0.001),
                device=device,
                early_stopping_patience=kwargs.get("early_stopping_patience", 5)
            )
        else:
            if train_labels is None:
                raise ValueError("train_labels required for non-torch models")
            self.model.fit(train_data, train_labels)

    def predict(self, x, device="cpu"):
        if isinstance(self.model, nn.Module):
            return self.model.predict(x, device=device)
        else:
            return self.model.predict(x)

# ================================
# 9. Evaluation Function
# ================================

def evaluate(classifier: MnistClassifier, loader, device="cpu"):
    correct, total = 0, 0
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        preds = classifier.predict(images, device=device)
        correct += (preds == labels).sum().item()
        total += labels.size(0)
    return correct / total

# ================================
# 10. Example Usage
# ================================

# ---- Random Forest ----
rf = RandomForestMnist()
rf.fit(X_train_rf, y_train_rf)

rf_preds = rf.predict(X_test_rf)
rf_acc = accuracy_score(y_test_rf, rf_preds)
print(f"Random Forest Accuracy: {rf_acc:.4f}")

print("\nClassification Report:")
print(classification_report(y_test_rf, rf_preds))

"""Random Forest serves as a baseline classical ML model.
The model performs very well with an overall accuracy of 97% on the test set.

The F1-scores for all classes range between 0.96 and 0.99, indicating balanced precision and recall across digits.

Some digits are easier to recognize, e.g., 0, 1, and 6 have high recall values (>0.98), meaning the model correctly identifies almost all instances of these digits.

Some digits are slightly more challenging, e.g., 9 has a recall of 0.95, showing that it is occasionally misclassified.
"""

device = "cuda" if torch.cuda.is_available() else "cpu"

# --- Feed-Forward Neural Network ---

nn_model = MnistClassifier("nn")
nn_model.fit(train_loader, val_data=test_loader, epochs=30, lr=0.001, device=device, early_stopping_patience=7)
nn_acc = evaluate(nn_model, test_loader, device=device)
print(f"Feed-Forward NN Accuracy: {nn_acc:.4f}")

"""Training loss steadily decreased from 0.2400 to 0.0246, indicating good learning of the patterns in the training set.

Validation loss reached its minimum at 0.0498, showing the model generalizes well to unseen data without significant overfitting.

The final accuracy on the test set is 98.54%.
"""

# --- Convolutional Neural Network ---

cnn_model = MnistClassifier("cnn")
cnn_model.fit(train_loader, val_data=test_loader, epochs=100, lr=0.0008, device=device, early_stopping_patience=10)
cnn_acc = evaluate(cnn_model, test_loader, device=device)
print(f"CNN Accuracy: {cnn_acc:.4f}")

"""## Results Summary

| Model | Test Accuracy |
|--------|----------------|
| Random Forest | 0.9706 |
| Feed-Forward NN | 0.9854 |
| CNN | 0.9956 |

CNN achieved the highest accuracy on MNIST, confirming the advantage of convolutional architectures for image data.

## Edge Cases

1. **Empty input tensor** → model should raise a clear error instead of crashing.  
2. **Wrong image size (e.g., 32×32)** → should be reshaped or rejected before training.  
3. **Too few epochs** → underfitting, accuracy <90%.  
4. **Too high learning rate** → unstable training, loss oscillates.  
5. **No early stopping** → risk of overfitting after ~20 epochs.
"""

# Edge case:

# 1. Empty input tensor — expect runtime error
try:
    cnn_model.predict(torch.tensor([]))
except Exception as e:
    print("Handled empty input:", e)

# 2. Wrong image size (32x32) — should trigger shape mismatch
wrong_input = torch.randn(1, 1, 32, 32)
try:
    cnn_model.predict(wrong_input)
except Exception as e:
    print("Handled wrong shape:", e)

# 3. Too high learning rate
small_cnn = MnistClassifier("cnn")
small_cnn.fit(train_loader, val_data=test_loader, epochs=3, lr=0.1, device=device)