{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Classifier Demo\n",
        "This notebook demonstrates the implementation of three MNIST classifiers:\n",
        "1. Random Forest\n",
        "2. Feed-Forward Neural Network\n",
        "3. Convolutional Neural Network (CNN)\n",
        "\n",
        "Each model implements the `MnistClassifierInterface` with methods:\n",
        "- `train`\n",
        "- `predict`"
      ],
      "metadata": {
        "id": "1oAwyOeveyrA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "GVgtw3emp4oM"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 1. Import libraries\n",
        "# ================================\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from abc import ABC, abstractmethod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "A-HiD9j3p744"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 2. Data preparation\n",
        "# ================================\n",
        "\n",
        "# Transform images to tensors for PyTorch models\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Download MNIST dataset\n",
        "train_dataset = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoader for NN and CNN\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# For RandomForest — convert to numpy arrays\n",
        "X_train_rf = train_dataset.data.view(-1, 28*28).numpy().astype(np.float32) / 255.0\n",
        "y_train_rf = train_dataset.targets.numpy().astype(np.int64)\n",
        "X_test_rf  = test_dataset.data.view(-1, 28*28).numpy().astype(np.float32) / 255.0\n",
        "y_test_rf  = test_dataset.targets.numpy().astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BSuZefVsrxkq"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 3. MnistClassifierInterface\n",
        "# ================================\n",
        "class MnistClassifierInterface(ABC):\n",
        "    @abstractmethod\n",
        "    def fit(self, train_data, train_labels=None):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, x):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voL-ps9s94g5",
        "outputId": "8dd6e08d-2c5b-486f-beb5-953cebb567c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "Найкращі параметри: {'n_estimators': 150, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'gini', 'bootstrap': True}\n",
            "Найкраща точність (CV): 0.9655666666666667\n",
            "Точність на тесті: 0.9706\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# 4. Random Forest Model with Randomized Search\n",
        "# ================================\n",
        "\n",
        "# Random search parameters\n",
        "param_dist = {\n",
        "    'n_estimators': [150, 200],\n",
        "    'max_depth': [None, 30],\n",
        "    'max_features': ['sqrt'],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Using RandomizedSearchCV is faster because it only tests n_iter combinations\n",
        "random_search = RandomizedSearchCV(estimator=rf,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=20,  # 20 random combinations\n",
        "                                   cv=3,\n",
        "                                   scoring='accuracy',\n",
        "                                   verbose=1,\n",
        "                                   n_jobs=-1,\n",
        "                                   random_state=42)\n",
        "\n",
        "# Model training\n",
        "random_search.fit(X_train_rf, y_train_rf)\n",
        "\n",
        "# Result\n",
        "print(\"The best parameters:\", random_search.best_params_)\n",
        "print(\"Best accuracy (CV):\", random_search.best_score_)\n",
        "\n",
        "# Test set score\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_rf)\n",
        "test_acc = accuracy_score(y_test_rf, y_pred)\n",
        "print(f\"Accuracy on the test: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XJfzs1idr3By"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Random Forest Implementation\n",
        "# ================================\n",
        "\n",
        "class RandomForestMnist(MnistClassifierInterface):\n",
        "    def __init__(self):\n",
        "        self.model = RandomForestClassifier(\n",
        "            n_estimators=150, min_samples_split=2, min_samples_leaf=1,\n",
        "            max_features='sqrt', max_depth= None,\n",
        "            criterion='gini', bootstrap=True,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    def fit(self, train_data, train_labels=None, **kwargs):\n",
        "        if train_labels is None:\n",
        "            raise ValueError(\"train_labels required for RandomForestMnist.fit\")\n",
        "        self.model.fit(train_data, train_labels)\n",
        "\n",
        "    def predict(self, x, **kwargs):\n",
        "        return self.model.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 5. Base class for trainable PyTorch models with training, validation, and early stopping\n",
        "# ================================\n",
        "\n",
        "class TrainableNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def fit(self, train_data, val_data=None, epochs=50, lr=0.001, device=\"cpu\", early_stopping_patience=5):\n",
        "        self.to(device)\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        best_model_wts = copy.deepcopy(self.state_dict())\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Train\n",
        "            self.train()\n",
        "            running_loss = 0.0\n",
        "            num_batches = 0\n",
        "            for images, labels in train_data:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self(images)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                num_batches += 1\n",
        "            train_loss = running_loss / max(1, num_batches)\n",
        "\n",
        "            # Validation\n",
        "            if val_data is not None:\n",
        "                self.eval()\n",
        "                val_loss = 0.0\n",
        "                val_batches = 0\n",
        "                with torch.no_grad():\n",
        "                    for images, labels in val_data:\n",
        "                        images, labels = images.to(device), labels.to(device)\n",
        "                        outputs = self(images)\n",
        "                        val_loss += loss_fn(outputs, labels).item()\n",
        "                        val_batches += 1\n",
        "                val_loss /= max(1, val_batches)\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}] — Train Loss: {train_loss:.4f} — Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "                # Save best\n",
        "                if val_loss < best_loss:\n",
        "                    best_loss = val_loss\n",
        "                    best_model_wts = copy.deepcopy(self.state_dict())\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= early_stopping_patience:\n",
        "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                        break\n",
        "            else:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}] — Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Restore best\n",
        "        self.load_state_dict(best_model_wts)\n",
        "        print(f\"\\nRestored best model (Val Loss: {best_loss:.4f})\")\n",
        "\n",
        "    def predict(self, x, device=\"cpu\"):\n",
        "        # Make predictions on a batch or full dataset\n",
        "        self.eval()\n",
        "        self.to(device)\n",
        "        x = x.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self(x)\n",
        "            return torch.argmax(outputs, dim=1)"
      ],
      "metadata": {
        "id": "DZtKGDW1lJgM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yzyaJi1zsA3m"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 6. Feed-Forward Neural Network\n",
        "# ================================\n",
        "\n",
        "class FeedForwardNN(TrainableNN):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3тя спроба покращити результат\n",
        "class CNNClassifierKerasStyle(TrainableNN):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # --- Convolutional layers ---\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # --- Pooling ---\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # --- Dropout ---\n",
        "        self.dropout_conv = nn.Dropout(0.25)\n",
        "        self.dropout_fc = nn.Dropout(0.3)\n",
        "\n",
        "        # --- Fully connected ---\n",
        "        self.fc1 = nn.Linear(128 * 7 * 7, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.dropout_fc2 = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.dropout_conv(x)\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.dropout_conv(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.dropout_fc2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AqNBcepJcfdz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "JF8FL4BesK3i"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 7. Convolutional Neural Network\n",
        "# ================================\n",
        "\n",
        "class CNNClassifierKerasStyle(TrainableNN):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Conv layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Pooling\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout_conv = nn.Dropout(0.25)\n",
        "        self.dropout_fc = nn.Dropout(0.5)\n",
        "\n",
        "        # Fully connected\n",
        "        self.fc1 = nn.Linear(128 * 7 * 7, 256)  # 128 канали × 7 × 7\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 28 → 14\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 14 → 7\n",
        "        x = F.relu(self.bn3(self.conv3(x)))             # 7 → 7\n",
        "        x = self.dropout_conv(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout_fc(x)\n",
        "        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4amd_ltPsPHt"
      },
      "outputs": [],
      "source": [
        "# ==================\n",
        "# 8. MnistClassifier\n",
        "# ==================\n",
        "\n",
        "class MnistClassifier:\n",
        "    def __init__(self, algorithm):\n",
        "        if algorithm == \"rf\":\n",
        "            self.model = RandomForestMnist()\n",
        "        elif algorithm == \"nn\":\n",
        "            self.model = FeedForwardNN()\n",
        "        elif algorithm == \"cnn\":\n",
        "            self.model = CNNClassifierKerasStyle()\n",
        "        else:\n",
        "            raise ValueError(\"Unknown algorithm\")\n",
        "\n",
        "    def fit(self, train_data, train_labels=None, **kwargs):\n",
        "        device = kwargs.get(\"device\", \"cpu\")\n",
        "        if isinstance(self.model, nn.Module):\n",
        "            self.model.to(device)\n",
        "            self.model.fit(\n",
        "                train_data,\n",
        "                val_data=kwargs.get(\"val_data\"),\n",
        "                epochs=kwargs.get(\"epochs\", 50),\n",
        "                lr=kwargs.get(\"lr\", 0.001),\n",
        "                device=device,\n",
        "                early_stopping_patience=kwargs.get(\"early_stopping_patience\", 5)\n",
        "            )\n",
        "        else:\n",
        "            if train_labels is None:\n",
        "                raise ValueError(\"train_labels required for non-torch models\")\n",
        "            self.model.fit(train_data, train_labels)\n",
        "\n",
        "    def predict(self, x, device=\"cpu\"):\n",
        "        if isinstance(self.model, nn.Module):\n",
        "            return self.model.predict(x, device=device)\n",
        "        else:\n",
        "            return self.model.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 9. Evaluation Function\n",
        "# ================================\n",
        "\n",
        "def evaluate(classifier: MnistClassifier, loader, device=\"cpu\"):\n",
        "    correct, total = 0, 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        preds = classifier.predict(images, device=device)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "LiGe0O_DiJlA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJpJof9ivoaP",
        "outputId": "e0a611cc-dd63-4055-e175-0002d824c5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.9706\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       980\n",
            "           1       0.99      0.99      0.99      1135\n",
            "           2       0.96      0.97      0.97      1032\n",
            "           3       0.96      0.96      0.96      1010\n",
            "           4       0.98      0.97      0.98       982\n",
            "           5       0.97      0.97      0.97       892\n",
            "           6       0.98      0.98      0.98       958\n",
            "           7       0.97      0.96      0.97      1028\n",
            "           8       0.96      0.96      0.96       974\n",
            "           9       0.96      0.95      0.96      1009\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# 10. Example Usage\n",
        "# ================================\n",
        "\n",
        "# ---- Random Forest ----\n",
        "rf = RandomForestMnist()\n",
        "rf.fit(X_train_rf, y_train_rf)\n",
        "\n",
        "rf_preds = rf.predict(X_test_rf)\n",
        "rf_acc = accuracy_score(y_test_rf, rf_preds)\n",
        "print(f\"Random Forest Accuracy: {rf_acc:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_rf, rf_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest serves as a baseline classical ML model.\n",
        "The model performs very well with an overall accuracy of 97% on the test set.\n",
        "\n",
        "The F1-scores for all classes range between 0.96 and 0.99, indicating balanced precision and recall across digits.\n",
        "\n",
        "Some digits are easier to recognize, e.g., 0, 1, and 6 have high recall values (>0.98), meaning the model correctly identifies almost all instances of these digits.\n",
        "\n",
        "Some digits are slightly more challenging, e.g., 9 has a recall of 0.95, showing that it is occasionally misclassified."
      ],
      "metadata": {
        "id": "BW3mjiabfOfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "hLgQv_xbUIgM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz57cOov3A37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb36b90a-dc32-48c9-9db9-9db51130be66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30] — Train Loss: 0.2400 — Val Loss: 0.1019\n",
            "Epoch [2/30] — Train Loss: 0.1212 — Val Loss: 0.0773\n",
            "Epoch [3/30] — Train Loss: 0.0967 — Val Loss: 0.0664\n",
            "Epoch [4/30] — Train Loss: 0.0793 — Val Loss: 0.0620\n",
            "Epoch [5/30] — Train Loss: 0.0687 — Val Loss: 0.0578\n",
            "Epoch [6/30] — Train Loss: 0.0651 — Val Loss: 0.0592\n",
            "Epoch [7/30] — Train Loss: 0.0561 — Val Loss: 0.0572\n",
            "Epoch [8/30] — Train Loss: 0.0499 — Val Loss: 0.0544\n",
            "Epoch [9/30] — Train Loss: 0.0478 — Val Loss: 0.0609\n",
            "Epoch [10/30] — Train Loss: 0.0424 — Val Loss: 0.0561\n",
            "Epoch [11/30] — Train Loss: 0.0424 — Val Loss: 0.0581\n",
            "Epoch [12/30] — Train Loss: 0.0377 — Val Loss: 0.0539\n",
            "Epoch [13/30] — Train Loss: 0.0356 — Val Loss: 0.0498\n",
            "Epoch [14/30] — Train Loss: 0.0329 — Val Loss: 0.0515\n",
            "Epoch [15/30] — Train Loss: 0.0316 — Val Loss: 0.0511\n",
            "Epoch [16/30] — Train Loss: 0.0297 — Val Loss: 0.0522\n",
            "Epoch [17/30] — Train Loss: 0.0280 — Val Loss: 0.0556\n",
            "Epoch [18/30] — Train Loss: 0.0252 — Val Loss: 0.0561\n",
            "Epoch [19/30] — Train Loss: 0.0275 — Val Loss: 0.0579\n",
            "Epoch [20/30] — Train Loss: 0.0246 — Val Loss: 0.0531\n",
            "Early stopping at epoch 20\n",
            "\n",
            "Restored best model (Val Loss: 0.0498)\n",
            "Feed-Forward NN Accuracy: 0.9854\n"
          ]
        }
      ],
      "source": [
        "# --- Feed-Forward Neural Network ---\n",
        "\n",
        "nn_model = MnistClassifier(\"nn\")\n",
        "nn_model.fit(train_loader, val_data=test_loader, epochs=30, lr=0.001, device=device, early_stopping_patience=7)\n",
        "nn_acc = evaluate(nn_model, test_loader, device=device)\n",
        "print(f\"Feed-Forward NN Accuracy: {nn_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loss steadily decreased from 0.2400 to 0.0246, indicating good learning of the patterns in the training set.\n",
        "\n",
        "Validation loss reached its minimum at 0.0498, showing the model generalizes well to unseen data without significant overfitting.\n",
        "\n",
        "The final accuracy on the test set is 98.54%."
      ],
      "metadata": {
        "id": "EU91D0XtfZln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Convolutional Neural Network ---\n",
        "\n",
        "cnn_model = MnistClassifier(\"cnn\")\n",
        "cnn_model.fit(train_loader, val_data=test_loader, epochs=100, lr=0.0008, device=device, early_stopping_patience=10)\n",
        "cnn_acc = evaluate(cnn_model, test_loader, device=device)\n",
        "print(f\"CNN Accuracy: {cnn_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeiCCEPkoNTi",
        "outputId": "4403f114-6b50-42f0-af53-e686ff295d0f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] — Train Loss: 0.1754 — Val Loss: 0.0556\n",
            "Epoch [2/100] — Train Loss: 0.0786 — Val Loss: 0.0274\n",
            "Epoch [3/100] — Train Loss: 0.0615 — Val Loss: 0.0250\n",
            "Epoch [4/100] — Train Loss: 0.0523 — Val Loss: 0.0241\n",
            "Epoch [5/100] — Train Loss: 0.0457 — Val Loss: 0.0269\n",
            "Epoch [6/100] — Train Loss: 0.0390 — Val Loss: 0.0212\n",
            "Epoch [7/100] — Train Loss: 0.0364 — Val Loss: 0.0175\n",
            "Epoch [8/100] — Train Loss: 0.0334 — Val Loss: 0.0178\n",
            "Epoch [9/100] — Train Loss: 0.0283 — Val Loss: 0.0178\n",
            "Epoch [10/100] — Train Loss: 0.0270 — Val Loss: 0.0202\n",
            "Epoch [11/100] — Train Loss: 0.0250 — Val Loss: 0.0168\n",
            "Epoch [12/100] — Train Loss: 0.0241 — Val Loss: 0.0180\n",
            "Epoch [13/100] — Train Loss: 0.0225 — Val Loss: 0.0155\n",
            "Epoch [14/100] — Train Loss: 0.0193 — Val Loss: 0.0179\n",
            "Epoch [15/100] — Train Loss: 0.0201 — Val Loss: 0.0220\n",
            "Epoch [16/100] — Train Loss: 0.0169 — Val Loss: 0.0165\n",
            "Epoch [17/100] — Train Loss: 0.0170 — Val Loss: 0.0174\n",
            "Epoch [18/100] — Train Loss: 0.0149 — Val Loss: 0.0176\n",
            "Epoch [19/100] — Train Loss: 0.0164 — Val Loss: 0.0180\n",
            "Epoch [20/100] — Train Loss: 0.0137 — Val Loss: 0.0215\n",
            "Epoch [21/100] — Train Loss: 0.0134 — Val Loss: 0.0182\n",
            "Epoch [22/100] — Train Loss: 0.0129 — Val Loss: 0.0170\n",
            "Epoch [23/100] — Train Loss: 0.0116 — Val Loss: 0.0162\n",
            "Early stopping at epoch 23\n",
            "\n",
            "Restored best model (Val Loss: 0.0155)\n",
            "CNN Accuracy: 0.9956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Summary\n",
        "\n",
        "| Model | Test Accuracy |\n",
        "|--------|----------------|\n",
        "| Random Forest | 0.9706 |\n",
        "| Feed-Forward NN | 0.9854 |\n",
        "| CNN | 0.9956 |\n",
        "\n",
        "CNN achieved the highest accuracy on MNIST, confirming the advantage of convolutional architectures for image data."
      ],
      "metadata": {
        "id": "8tokGJlJfUoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edge Cases\n",
        "\n",
        "1. **Empty input tensor** → model should raise a clear error instead of crashing.  \n",
        "2. **Wrong image size (e.g., 32×32)** → should be reshaped or rejected before training.  \n",
        "3. **Too few epochs** → underfitting, accuracy <90%.  \n",
        "4. **Too high learning rate** → unstable training, loss oscillates.  \n",
        "5. **No early stopping** → risk of overfitting after ~20 epochs."
      ],
      "metadata": {
        "id": "cOalJD3WfHHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edge case:\n",
        "\n",
        "# 1. Empty input tensor — expect runtime error\n",
        "try:\n",
        "    cnn_model.predict(torch.tensor([]))\n",
        "except Exception as e:\n",
        "    print(\"Handled empty input:\", e)"
      ],
      "metadata": {
        "id": "qMIpEAG2j4DM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9a5d85-49c4-401e-e884-46e52826c275"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Handled empty input: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Wrong image size (32x32) — should trigger shape mismatch\n",
        "wrong_input = torch.randn(1, 1, 32, 32)\n",
        "try:\n",
        "    cnn_model.predict(wrong_input)\n",
        "except Exception as e:\n",
        "    print(\"Handled wrong shape:\", e)"
      ],
      "metadata": {
        "id": "1thdyG7Ok1gI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5be5a6b-5ed3-454c-b375-be16925da7cc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Handled wrong shape: mat1 and mat2 shapes cannot be multiplied (1x8192 and 6272x256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Too high learning rate\n",
        "small_cnn = MnistClassifier(\"cnn\")\n",
        "small_cnn.fit(train_loader, val_data=test_loader, epochs=3, lr=0.1, device=device)"
      ],
      "metadata": {
        "id": "--InoGa3k3qZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76fa1fe9-46d6-4138-fdf3-7afe3f594dde"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3] — Train Loss: 6.1067 — Val Loss: 2.3206\n",
            "Epoch [2/3] — Train Loss: 2.3103 — Val Loss: 2.3109\n",
            "Epoch [3/3] — Train Loss: 2.3095 — Val Loss: 2.3111\n",
            "\n",
            "Restored best model (Val Loss: 2.3109)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}